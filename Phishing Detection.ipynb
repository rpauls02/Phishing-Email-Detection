{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s753yfbCse6l"
      },
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KASw2w1OUmmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e547d3-0afd-4921-a4ca-b8e34d229371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "import time\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQNa0GGkFdHG"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XBx4AQWFbiw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968f6303-4ab2-42ec-8ee1-845679ce77da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Dataset', 'Resources', 'epoch_loss.png', 'Submission', 'Phishing Detection.ipynb']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'City, University of London - Laptop/MSci Computer Science/Year 3/Semester 2/IN3045 Natural Language Processing/Coursework'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIfQNN5Mscjk"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTdxxXmJL51Q"
      },
      "source": [
        "## Understanding Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sf9WGtI1NY4"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "email_dataset = os.path.join(GOOGLE_DRIVE_PATH, 'Dataset/emails.csv')\n",
        "email_df = pd.read_csv(email_dataset)\n",
        "\n",
        "# Get information about dataset\n",
        "#print(email_df.info())\n",
        "\n",
        "# Check data types\n",
        "#print('\\nData types:')\n",
        "#print(email_df.dtypes, '\\n')\n",
        "\n",
        "# Check for duplicate values\n",
        "#print('Duplicate values:', email_df.duplicated().sum(), '\\n')\n",
        "\n",
        "# Check for null values\n",
        "#print('Null values:')\n",
        "#print(email_df.isnull().sum())\n",
        "\n",
        "# Check label frequencies\n",
        "#print('\\nLabel frequencies:')\n",
        "#print(email_df['Email Type'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmYBRyj9L95o"
      },
      "source": [
        "## Cleaning data\n",
        "\n",
        "* Text converted to lowercase\n",
        "* Null and duplicate values removed\n",
        "* 'empty' email bodies removed\n",
        "* Line breaks, non-word and digit characters, and stopwords removed\n",
        "* Reset index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRrT1HXiLrSc"
      },
      "outputs": [],
      "source": [
        "# Regular expressions for preprocessing\n",
        "line_break_regex = r'\\n'\n",
        "non_word_regex = r'[^a-zA-Z]+'\n",
        "\n",
        "# Convert all text to lowercase\n",
        "email_df.loc[:, 'Email Text'] = email_df['Email Text'].str.lower()\n",
        "\n",
        "# Remove null values\n",
        "email_df = email_df.dropna(axis=0)\n",
        "\n",
        "# Remove duplicate values\n",
        "email_df = email_df.drop_duplicates()\n",
        "\n",
        "# Remove empty 'Email Text' cells\n",
        "# From manual review of the dataset, some Email Text cells are filled with\n",
        "# 'empty' to indicate no email body is present\n",
        "email_df = email_df[email_df['Email Text'] != 'empty']\n",
        "\n",
        "# Remove line breaks\n",
        "email_df['Email Text'] = email_df['Email Text'].str.replace(line_break_regex,\n",
        "                                                            ' ', regex=True)\n",
        "# Remove non-word and digit characters\n",
        "email_df['Email Text'] = email_df['Email Text'].str.replace(non_word_regex,\n",
        "                                                            ' ', regex=True)\n",
        "# Remove stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "email_df['Email Text'] = email_df['Email Text'].apply(lambda text: ' '.join(\n",
        "    [word for word in text.split() if word.lower() not in STOPWORDS]))\n",
        "\n",
        "# Convert categorical labels to numerical\n",
        "email_df['Email Type'] = email_df['Email Type'].replace({'Safe Email' : 0,\n",
        "                                                         'Phishing Email' : 1})\n",
        "\n",
        "# Remove unnamed column containing row number\n",
        "email_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "# Reset index after preprocessing\n",
        "email_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print('\\nData after preprocessing')\n",
        "print(email_df.info(), '\\n')\n",
        "\n",
        "# Save cleaned dataset\n",
        "cleaned_df_path = os.path.join(GOOGLE_DRIVE_PATH, 'Dataset/emails_cleaned.csv')\n",
        "email_df.to_csv((cleaned_df_path), index=False, mode='w')\n",
        "if (os.path.exists(cleaned_df_path)):\n",
        "    print('Cleaned dataset saved at:')\n",
        "    print(cleaned_df_path)\n",
        "else:\n",
        "    print('Couldn\\'t save cleaned dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqbN54uOjMrm"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V8fqp6qvi73"
      },
      "outputs": [],
      "source": [
        "# Load cleaned dataset\n",
        "cleaned_dataset = os.path.join(GOOGLE_DRIVE_PATH, 'Dataset/emails_cleaned.csv')\n",
        "cleaned_df = pd.read_csv(cleaned_dataset)\n",
        "\n",
        "corpus = []\n",
        "for text in cleaned_df['Email Text']:\n",
        "    tokens = word_tokenize(str(text))\n",
        "    tokens = [token for token in tokens if len(wordnet.synsets(token)) > 0]\n",
        "    corpus.append(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xo7jXJSsRNI"
      },
      "source": [
        "# Baseline Models\n",
        "\n",
        "Baseline models include Decision Tree, Support Vector Machine and Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJrsEMnfMnzV"
      },
      "outputs": [],
      "source": [
        "# Vectorise corpus\n",
        "vect = CountVectorizer(binary=True)\n",
        "vect_corpus = vect.fit_transform(corpus)\n",
        "\n",
        "# Collect features and labels\n",
        "X = vect_corpus\n",
        "y = cleaned_df['Email Type']\n",
        "\n",
        "# Generate training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "models = {'Decision Tree' : DecisionTreeClassifier(\n",
        "                            criterion='gini', max_depth=100, max_features=100,\n",
        "                            max_leaf_nodes=50, min_samples_leaf=50,\n",
        "                            min_samples_split=50, random_state=42),\n",
        "          'SVC' : SVC(kernel='rbf', C=3.0, gamma='scale'),\n",
        "          'MultinomialNB' : MultinomialNB()}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Train models\n",
        "    t0 = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f'Model: {model_name}')\n",
        "    print(f'Training time: {time.time() - t0:.3f} seconds')\n",
        "\n",
        "    # Test models\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Generate statistics\n",
        "    print(f'Classification report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print('=======================================================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djV0eIKhtqe-"
      },
      "source": [
        "# Generate Word Embeddings\n",
        "\n",
        "Word embeddings are generated using CBOW Word2Vec with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjTOkq4nXtQH"
      },
      "outputs": [],
      "source": [
        "# Building vocabulary\n",
        "word_counts = Counter([word for email in corpus for word in email.split()])\n",
        "#print('\\nword_counts:', word_counts)\n",
        "vocabulary = list(word_counts.keys())\n",
        "#print('\\nvocabulary:', vocabulary)\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "#print('\\nword_to_index:', word_to_index)\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_size = 150\n",
        "window_size = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocabulary_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embedded_context = self.embedding(context)\n",
        "        sum_embedded_context = torch.sum(embedded_context, dim=0)\n",
        "        output = self.linear(sum_embedded_context)\n",
        "        return output\n",
        "\n",
        "def train_cbow(corpus, word_to_index, vocabulary, embedding_size, window_size,\n",
        "               learning_rate, num_epochs, batch_size):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(device)\n",
        "\n",
        "    corpus_tensors = []\n",
        "    for email in corpus:\n",
        "        for i in range(window_size, len(email.split()) - window_size):\n",
        "            context = [word_to_index[word] for word in\n",
        "                       email.split()[i-window_size:i] +\n",
        "                       email.split()[i+1:i+window_size+1]]\n",
        "\n",
        "            target = word_to_index[email.split()[i]]\n",
        "            corpus_tensors.append((context, target))\n",
        "\n",
        "    dataloader = DataLoader(corpus_tensors, batch_size=batch_size, shuffle=True)\n",
        "    model = CBOW(len(vocabulary), embedding_size).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "        for contexts, targets in dataloader:\n",
        "            context_tensor = torch.stack(\n",
        "                [torch.tensor(context, dtype=torch.long) for context in\n",
        "                 contexts]).to(device)\n",
        "            target_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(context_tensor)\n",
        "            loss = criterion(output, target_tensor)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_time = end_time - start_time\n",
        "        print(\"Epoch %s - loss=%.4f | time=%.2f seconds\"\n",
        "              % (epoch+1, total_loss, epoch_time))\n",
        "    return model\n",
        "\n",
        "# Train CBOW model\n",
        "model = train_cbow(corpus[:7240], word_to_index, vocabulary, embedding_size,\n",
        "                   window_size, learning_rate, num_epochs, batch_size)\n",
        "\n",
        "# Collect learned embeddings\n",
        "word_embeddings = model.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "# Save embeddings\n",
        "def save_embeddings(embeddings, vocabulary, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for i, word in enumerate(vocabulary):\n",
        "            embedding = ' '.join(map(str, embeddings[i]))\n",
        "            file.write(f\"{word} {embedding}\\n\")\n",
        "\n",
        "save_embeddings(word_embeddings, vocabulary,\n",
        "                os.path.join(GOOGLE_DRIVE_PATH, 'word_embeddings.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj0p2fh1Lhwf"
      },
      "source": [
        "# Primary Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rECtjgLstwl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c993867-7bdd-4dff-febc-69c75268ccc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest model\n",
            "Training time: 21.095 seconds\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94      2237\n",
            "           1       0.94      0.87      0.90      1384\n",
            "\n",
            "    accuracy                           0.93      3621\n",
            "   macro avg       0.93      0.92      0.92      3621\n",
            "weighted avg       0.93      0.93      0.93      3621\n",
            "\n",
            "=======================================================\n",
            "MLP model\n",
            "Training time: 25.242 seconds\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      2237\n",
            "           1       0.95      0.93      0.94      1384\n",
            "\n",
            "    accuracy                           0.96      3621\n",
            "   macro avg       0.96      0.95      0.95      3621\n",
            "weighted avg       0.96      0.96      0.96      3621\n",
            "\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "'''Generate train/test sets for word embeddings'''\n",
        "cleaned_dataset = os.path.join(GOOGLE_DRIVE_PATH, 'Dataset/emails_cleaned.csv')\n",
        "cleaned_df = pd.read_csv(cleaned_dataset)\n",
        "embeddings_path = os.path.join(GOOGLE_DRIVE_PATH, 'word_embeddings.txt')\n",
        "\n",
        "def load_word_embeddings(embeddings_path):\n",
        "    embeddings = {}\n",
        "    with open(embeddings_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            embedding = np.array([float(val) for val in values[1:]])\n",
        "            embeddings[word] = embedding\n",
        "    return embeddings\n",
        "\n",
        "def aggregate_embeddings(text, embeddings):\n",
        "    words = str(text).split()\n",
        "    email_embedding = np.zeros(len(next(iter(embeddings.values()))))\n",
        "    count = 0\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            email_embedding += embeddings[word]\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        email_embedding /= count\n",
        "    return email_embedding\n",
        "\n",
        "embeddings = load_word_embeddings(embeddings_path)\n",
        "\n",
        "X_embd = np.array([aggregate_embeddings(text, embeddings) for\n",
        "                   text in cleaned_df['Email Text']])\n",
        "y = cleaned_df['Email Type']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train_embd, X_test_embd, y_train_embd, y_test_embd = train_test_split(\n",
        "    X_embd, y, test_size=0.2, random_state=42)\n",
        "\n",
        "models = {'Random Forest' : RandomForestClassifier(),\n",
        "          'MLP' : MLPClassifier()}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Train models\n",
        "    t0 = time.time()\n",
        "    model.fit(X_train_embd, y_train_embd)\n",
        "    print(f'{model_name} model')\n",
        "    print(f'Training time: {time.time() - t0:.3f} seconds')\n",
        "\n",
        "    # Test models\n",
        "    y_pred = model.predict(X_test_embd)\n",
        "\n",
        "    # Generate statistics\n",
        "    print(f'Classification report:')\n",
        "    print(classification_report(y_test_embd, y_pred))\n",
        "    print('=======================================================')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s753yfbCse6l",
        "uQNa0GGkFdHG",
        "RmYBRyj9L95o",
        "djV0eIKhtqe-"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}